{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PA_L9X3FQukw",
        "outputId": "9f7a8cfd-3ac9-4e13-fce7-ec9fb3e91e0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [1 InRelease 3,632 \u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to r2u.s\u001b[0m\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,683 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,972 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,387 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,944 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,730 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.6 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,552 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,540 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,255 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Fetched 32.0 MB in 3s (11.4 MB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "93 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pci.ids pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 93 not upgraded.\n",
            "Need to get 343 kB of archives.\n",
            "After this operation, 1,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 pci.ids all 0.0~2022.01.22-1ubuntu0.1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Fetched 343 kB in 1s (323 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1ubuntu0.1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!sudo apt install pciutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VttPuKTyRWtC",
        "outputId": "e1144671-a98e-4da5-c55a-422fc5ee28c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat May 24 12:30:01 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "019SKlZ_UfLu",
        "outputId": "1a223a97-5faa-4dd4-f1c1-584a50d5c458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# install ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "o5VWuN9STs06"
      },
      "outputs": [],
      "source": [
        "!pkill -f ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg97mUsv1qmR",
        "outputId": "ff65dad8-8c25-4df1-9ff7-a37e9ba4ae75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "time=2025-05-24T12:33:47.817Z level=INFO source=routes.go:1258 msg=\"Listening on [::]:8000 (version 0.7.1)\"\n",
            "time=2025-05-24T12:33:47.817Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
            "time=2025-05-24T12:33:48.011Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-15e434bc-f0c0-4771-483a-e39c98ee563b library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n",
            "Error: listen tcp 0.0.0.0:8000: bind: address already in use\n",
            "time=2025-05-24T12:40:15.540Z level=INFO source=routes.go:1205 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:8000 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2025-05-24T12:40:15.540Z level=INFO source=images.go:463 msg=\"total blobs: 0\"\n",
            "time=2025-05-24T12:40:15.540Z level=INFO source=images.go:470 msg=\"total unused blobs removed: 0\"\n",
            "time=2025-05-24T12:40:15.541Z level=INFO source=routes.go:1258 msg=\"Listening on [::]:8000 (version 0.7.1)\"\n",
            "time=2025-05-24T12:40:15.541Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
            "time=2025-05-24T12:40:15.674Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-15e434bc-f0c0-4771-483a-e39c98ee563b library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n"
          ]
        }
      ],
      "source": [
        "# Setting the Model ID\n",
        "ollama_model_id = \"qwen3:4b\"\n",
        "\n",
        "# Starting the Ollama Service\n",
        "!nohup bash -c \"OLLAMA_HOST=0.0.0.0:8000 OLLAMA_ORIGIN=* ollama serve\" &\n",
        "!sleep 50 && tail /content/nohup.out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrW2i05GSbo6",
        "outputId": "c738a1ac-89a1-41cd-a69f-4b19509f58bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "pulling eb4402837c78: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\n",
            "pulling d18a5cc71b84: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
            "pulling cff3f395ef37: 100% ▕██████████████████▏  120 B                         \u001b[K\n",
            "pulling 5efd52d6d9f2: 100% ▕██████████████████▏  487 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n",
            "[GIN] 2025/05/24 - 12:43:34 | 200 |   39.416907ms |       127.0.0.1 | POST     \"/api/show\"\n",
            "[GIN] 2025/05/24 - 12:43:34 | 200 |   19.537708ms |       127.0.0.1 | POST     \"/api/generate\"\n",
            "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Running the Ollama Model\n",
        "!nohup bash -c \"OLLAMA_HOST=0.0.0.0:8000 OLLAMA_ORIGIN=* ollama run {ollama_model_id}\"  &\n",
        "!sleep 50 && tail /content/nohup.out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqvaowCEUXdr",
        "outputId": "3dd15c52-9ce3-4936-daa0-7a94da0a349d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"model\":\"qwen3:4b\",\"created_at\":\"2025-05-24T12:44:54.795849205Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\u003cthink\\u003e\\nOkay, the user is asking for the capital of Egypt. I know that Cairo is the capital. But wait, I should make sure there's no confusion with other major cities. Alexandria is a big city, but it's not the capital. Luxor is a historical site, but again, not the capital. So the answer is definitely Cairo. I should mention that it's the capital and maybe add some context, like it's the largest city and a cultural hub. Also, check if there's any other city that might be considered, but I don't think so. Yeah, Cairo is the correct answer.\\n\\u003c/think\\u003e\\n\\nعاصمة مصر هي **القاهرة**.她是埃及最大的城市，也是政治、经济和文化中心。القاهرة تقع في جنوب مصر، وتضم العديد من المعالم التاريخية مثل متحف الباري، وقلعة الصفا، ونهر النيل. كما تُعتبر مركزًا للحياة التجارية والتعليم.\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":4616152116,\"load_duration\":31665809,\"prompt_eval_count\":14,\"prompt_eval_duration\":495647307,\"eval_count\":204,\"eval_duration\":4087516649}"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   127    0     0  100   127      0    105  0:00:01  0:00:01 --:--:--   105\r100   127    0     0  100   127      0     57  0:00:02  0:00:02 --:--:--    57\r100   127    0     0  100   127      0     39  0:00:03  0:00:03 --:--:--    39\r100   127    0     0  100   127      0     30  0:00:04  0:00:04 --:--:--    30\r100  1376  100  1249  100   127    270     27  0:00:04  0:00:04 --:--:--   297\r100  1376  100  1249  100   127    270     27  0:00:04  0:00:04 --:--:--   365\n"
          ]
        }
      ],
      "source": [
        "#Calling the Ollama API\n",
        "%%bash\n",
        "curl http://localhost:8000/api/chat -d '{\n",
        "  \"model\": \"qwen3:4b\",\n",
        "  \"stream\": false,\n",
        "  \"messages\": [\n",
        "    { \"role\": \"user\", \"content\": \"ما عاصمة مصر؟\" }\n",
        "  ]\n",
        "}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LuFj6ZDVcDq",
        "outputId": "92e612b7-d0ce-4232-aaab-d7166c02f95b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK4-WJfAVVux",
        "outputId": "425a258a-7639-4609-f6b0-a4026e87c081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://2d05-34-125-250-199.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "ngrok_auth = userdata.get('colab-ngrok')\n",
        "\n",
        "conf.get_default().auth_token = ngrok_auth\n",
        "\n",
        "port = \"8000\"\n",
        "\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3NPa0DxVvXe",
        "outputId": "3e0af36d-52a2-456f-a00f-d9b35c4988b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"model\":\"qwen3:4b\",\"created_at\":\"2025-05-24T12:51:00.972152297Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\\u003cthink\\u003e\\nOkay, the user is asking for the capital of Egypt. I need to make sure I provide the correct answer. I know that Cairo is the capital, but I should double-check to be certain. Let me recall: Egypt's capital is indeed Cairo, which is also the largest city in the country. I should mention that it's the political, cultural, and economic center. Maybe also note that it's home to important landmarks like the Pyramids of Giza and the Egyptian Museum. I should keep the answer straightforward and confirm the information to avoid any mistakes.\\n\\u003c/think\\u003e\\n\\nعاصمة مصر هي **القاهرة**.  \\nتُعتبر القاهرة مركزًا ثقافيًا واقتصاديًا وسياسًا للبلاد، وتقع في الجنوب الغربي لشبه جزيرة سيناء.  \\nمن أبرز معالمها:  \\n- **الهرم الكبير** (الهرم البهيموي) و**الهرم البحري** وال**هرم الأبيض** في منطقة الهرم.  \\n- **متحف مصر** في القاهرة.  \\n- **البُرج الذهبي** (البُرج الهوائي).  \\n- **الخطابات والمعالم التاريخية** مثل متحف الفنون والكالdera.  \\n\\nالقاهرة تُعد واحدة من أبرز المدن في العالم، وتقع على ضفاف النهر الأبيض.\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":8154003376,\"load_duration\":2032990932,\"prompt_eval_count\":14,\"prompt_eval_duration\":167367510,\"eval_count\":298,\"eval_duration\":5952705475}"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   127    0     0  100   127      0    578 --:--:-- --:--:-- --:--:--   577\r100   127    0     0  100   127      0    103  0:00:01  0:00:01 --:--:--   103\r100   127    0     0  100   127      0     57  0:00:02  0:00:02 --:--:--    57\r100   127    0     0  100   127      0     39  0:00:03  0:00:03 --:--:--    39\r100   127    0     0  100   127      0     30  0:00:04  0:00:04 --:--:--    30\r100   127    0     0  100   127      0     24  0:00:05  0:00:05 --:--:--     0\r100   127    0     0  100   127      0     20  0:00:06  0:00:06 --:--:--     0\r100   127    0     0  100   127      0     17  0:00:07  0:00:07 --:--:--     0\r100   127    0     0  100   127      0     15  0:00:08  0:00:08 --:--:--     0\r100  1772  100  1645  100   127    194     14  0:00:09  0:00:08  0:00:01   387\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "curl https://2d05-34-125-250-199.ngrok-free.app/api/chat -d '{\n",
        "  \"model\": \"qwen3:4b\",\n",
        "  \"stream\": false,\n",
        "  \"messages\": [\n",
        "    { \"role\": \"user\", \"content\": \"ما عاصمة مصر؟\" }\n",
        "  ]\n",
        "}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa5cndpPV7Kp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "LLMS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
